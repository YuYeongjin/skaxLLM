# -*- coding: utf-8 -*-
"""LangGraph_RAG_yjyoo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18-bXgC_5F58Nf6IPuUzCNzm4p6vfosyd
"""

!pip install numpy==1.26.4
!pip install langchain==0.3.21
!pip install langchain-core==0.3.46
!pip install langchain-experimental==0.3.4
!pip install langchain-community==0.3.20
!pip install langchain-openai==0.3.9
!pip install langchain-chroma==0.2.2
!pip install langchain-cohere==0.4.3
#!pip install langchain-milvus==0.1.8
!pip install langgraph==0.3.18
#!pip install langsmith==0.3.18
!pip install pymupdf==1.25.4
!pip install pypdf==4.3.1
!pip install pdfplumber==0.11.5
!pip install faiss-cpu==1.10.0
!pip install langchain-teddynote==0.3.44

import os
os.environ["OPENAI_API_KEY"]=""
os.environ["TAVILY_API_KEY"]=""

from pdf import PDFRetrievalChain
from utils import format_docs
pdf = PDFRetrievalChain(["/content/SPRI_AI_Brief_2023년12월호_F.pdf"]).create_chain()
pdf_retriever = pdf.retriever
pdf_chain = pdf.chain
#search_result = pdf_retriever.invoke("앤트로픽 투자한 기업은?") # 문서 리스트

#검색 결과를 기반으로 답변을 생성
answer = pdf_chain.invoke(
   {
      "question":"앤트로픽에 투자한 기업과 투자금액은?",
     "context": search_result,
       "chat_history":[],
   }
)
print(answer)

from typing import Annotated, TypedDict
from langgraph.graph.message import add_messages

# State 정의
class GraphState(TypedDict):
  answer: Annotated[str, "Answer"] # 답변
  context: Annotated[str, "Context"] # 문서의 검색 결과
  question:Annotated[str, "Question"] # 질문
  messages:Annotated[list,add_messages] # 메시지 ( 누적되는 Message)

# 문서 검색노드
def retrieve_documents(state:GraphState)->GraphState:
  last_question = state["question"]
  retrieved_docs = pdf_retriever.invoke(last_question)
  # List 로 결과가 오는데, Document들을 context로 return 하면 1개로 인식할 수 있기 때문에 분리
  retrieved_docs = format_docs(retrieved_docs)
  return GraphState(context=retrieved_docs)

# 답변 생성노드
def llm_answer(state:GraphState)->GraphState:
  latest_question = state["question"]
  context = state["context"]
  answer = pdf_chain.invoke(
    {
        "question":latest_question,
        "context": context,
        "chat_history":messages_to_history(state["messages"]),
    }
  )
  return GraphState(answer=answer,messages=[("user",latest_question),("assistant",answer)])

from IPython.display import Image,display
def show_graph(graph):
  try:
    display( Image(graph.get_graph().draw_mermaid_png()))

  except Exception:
    pass

from langgraph.graph import END, StateGraph
from langgraph.checkpoint.memory import MemorySaver

workflow1 = StateGraph(GraphState)
workflow1.add_node("retrieve",retrieve_documents)
workflow1.add_node("llm_answer",llm_answer)

workflow1.add_edge("retrieve","llm_answer")
workflow1.add_edge("llm_answer",END)

workflow1.set_entry_point("retrieve")

memory = MemorySaver()
app = workflow1.compile(checkpointer= memory)
show_graph(app)

from langchain_core.runnables import RunnableConfig
from langchain_teddynote.messages import invoke_graph,stream_graph,random_uuid

# config 설정 (재귀 최대 횟수, thread_id)
config = RunnableConfig(recursion_limit=20, configurable={"thread_id":random_uuid})

# Questions
inputs = GraphState(question="앤스로픽에 투자한 기업과 투자금액을 알려줘")
# invoke_graph
invoke_graph(app, inputs, config, ["llm_answer"])

"""## 질문과 검색한 문서에 관한 적정성 검사 노드 추가"""

from typing import Annotated, TypedDict
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from langchain_teddynote.messages import messages_to_history

# State 정의
class GraphState(TypedDict):
  answer: Annotated[str, "Answer"] # 답변
  context: Annotated[str, "Context"] # 문서의 검색 결과
  question:Annotated[str, "Question"] # 질문
  messages:Annotated[list,add_messages] # 메시지 ( 누적되는 Message)
  relevance: Annotated[str,"Relevance"] # 관련성

  # 문서 검색노드
def retrieve_documents(state:GraphState)->GraphState:
  last_question = state["question"]
  retrieved_docs = pdf_retriever.invoke(last_question)
  retrieved_docs = format_docs(retrieved_docs)
  return GraphState(context=retrieved_docs)

# 답변 생성노드
def llm_answer(state:GraphState)->GraphState:
  latest_question = state["question"]
  context = state["context"]
  answer = pdf_chain.invoke(
    {
        "question":latest_question,
        "context": context,
        "chat_history":messages_to_history(state["messages"]),
    }
  )
  return GraphState(answer=answer,messages=[("user",latest_question),("assistant",answer)])

########################################

# 적정성 평가 노드 정의
from langchain_teddynote.evaluator import GroundednessChecker
def relevance_check(state:GraphState)->GraphState:
  # 생성
  checker = GroundednessChecker(llm=ChatOpenAI(model="gpt-4.1-nano",temperature=0), target="question-retrieval").create()
  # 호출
  response = checker.invoke({"question":state["question"], "context":state["context"]})
  print(response.score)
  return {"relevance":response.score}

def is_relevant(state:GraphState):
  if state["relevance"]=="yes":
    return "relevant"
  else:
    return "not relevant"

from langgraph.graph import END, StateGraph
from langgraph.checkpoint.memory import MemorySaver

workflow2 = StateGraph(GraphState)
workflow2.add_node("retrieve",retrieve_documents)
workflow2.add_node("llm_answer",llm_answer)
workflow2.add_node("relevance_check",relevance_check)

workflow2.add_edge("retrieve","relevance_check")

workflow2.add_edge("llm_answer",END)

workflow2.add_conditional_edges(
    "relevance_check",
    is_relevant,
    {"relevant":"llm_answer","not relevant":"retrieve"}
)

workflow2.set_entry_point("retrieve")

memory2 = MemorySaver()
app2 = workflow2.compile(checkpointer= memory2)
show_graph(app2)

from langchain_core.runnables import RunnableConfig
from langchain_teddynote.messages import invoke_graph,stream_graph,random_uuid

# config 설정 (재귀 최대 횟수, thread_id)
config = RunnableConfig(recursion_limit=20, configurable={"thread_id":random_uuid})

# Questions
inputs = GraphState(question="2025년 앤트로픽 투자 회사는?")
# invoke_graph
invoke_graph(app2, inputs, config, ["llm_answer"])

# 도구 초기화
from langchain_community.tools.tavily_search import TavilySearchResults
from langgraph.prebuilt import ToolNode, tools_condition
# 검색도구 생성
tool = TavilySearchResults(max_result = 3)

tools = [tool]

from typing import Annotated, TypedDict
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from langchain_teddynote.messages import messages_to_history

# State 정의
class GraphState(TypedDict):
  answer: Annotated[str, "Answer"] # 답변
  context: Annotated[str, "Context"] # 문서나 웹의 검색 결과
  question:Annotated[str, "Question"] # 질문
  messages:Annotated[list,add_messages] # 메시지 ( 누적되는 Message)
  relevance: Annotated[str,"Relevance"] # 관련성

  # 문서 검색노드
def retrieve_documents(state:GraphState)->GraphState:
  last_question = state["question"]
  retrieved_docs = pdf_retriever.invoke(last_question)
  retrieved_docs = format_docs(retrieved_docs)
  return GraphState(context=retrieved_docs)

# 답변 생성노드
def llm_answer(state:GraphState)->GraphState:
  latest_question = state["question"]
  context = state["context"]
  answer = pdf_chain.invoke(
    {
        "question":latest_question,
        "context": context,
        "chat_history":messages_to_history(state["messages"]),
    }
  )
  return GraphState(answer=answer,messages=[("user",latest_question),("assistant",answer)])

# 적정성 평가 노드 정의
from langchain_teddynote.evaluator import GroundednessChecker
def relevance_check(state:GraphState)->GraphState:
  # 생성
  checker = GroundednessChecker(llm=ChatOpenAI(model="gpt-4.1-nano",temperature=0), target="question-retrieval").create()
  # 호출
  response = checker.invoke({"question":state["question"], "context":state["context"]})
  print(response.score)
  return {"relevance":response.score}

def is_relevant(state:GraphState):
  if state["relevance"]=="yes":
    return "relevant"
  else:
    return "not relevant"


########################################
# Web Search 노드 추가

def web_search(state:GraphState)->GraphState:
  q= state["question"]
  result = tool.invoke(q)
  return {"context":result}

from langgraph.graph import END, StateGraph
from langgraph.checkpoint.memory import MemorySaver

workflow3 = StateGraph(GraphState)
workflow3.add_node("retrieve",retrieve_documents)
workflow3.add_node("llm_answer",llm_answer)
workflow3.add_node("relevance_check",relevance_check)
workflow3.add_node("web_search",web_search)
workflow3.add_edge("retrieve","relevance_check")

workflow3.add_edge("llm_answer",END)

workflow3.add_conditional_edges(
    "relevance_check",
    is_relevant,
    {"relevant":"llm_answer","not relevant":"web_search"}
)
workflow3.add_edge("web_search","llm_answer")
workflow3.set_entry_point("retrieve")

memory3 = MemorySaver()
app3 = workflow3.compile(checkpointer= memory2)
show_graph(app3)

from langchain_core.runnables import RunnableConfig
from langchain_teddynote.messages import invoke_graph,stream_graph,random_uuid

# config 설정 (재귀 최대 횟수, thread_id)
config = RunnableConfig(recursion_limit=20, configurable={"thread_id":random_uuid})

# Questions
inputs = GraphState(question="2025년 대한민국 총 인구 수는?")
# invoke_graph
invoke_graph(app3, inputs, config, ["llm_answer"])

from langchain_core.runnables import RunnableConfig
from langchain_teddynote.messages import invoke_graph,stream_graph,random_uuid

# config 설정 (재귀 최대 횟수, thread_id)
config = RunnableConfig(recursion_limit=20, configurable={"thread_id":random_uuid})

# Questions
inputs = GraphState(question="생성형 AI 가우스를 만든 회사의 2024 매출 실적은?")
# invoke_graph
invoke_graph(app3, inputs, config, ["llm_answer"])

"""## Reg Agent 구축"""

# 리트리버를 에이전트의 도구 세팅
from langchain_core.tools.retriever import create_retriever_tool
from langchain_core.prompts import PromptTemplate
# PDF 문서를 기반으로 검색 도구 생성
retriever_tool = create_retriever_tool(
    pdf_retriever,
    "pdf_retriever",
    "Search and return information about SPRI AI Brief PDF file. It contains useful information on recent AI trends. The document is published on Dec 2023.",
    document_prompt=PromptTemplate.from_template(
        "<document><context>{page_content}</context><metadata><source>{source}</source><page>{page}</page></metadata></document>"
    ),
)
#생성된 검색도구를 도구 리스트에 추가하여 에이전트에서 사용 가능하도록 설정
tools = [retriever_tool]

from typing import Annotated,TypedDict
from langgraph.graph import StateGraph,START,END
from langgraph.graph.message import add_messages
from langchain_openai import ChatOpenAI
from langgraph.prebuilt import ToolNode, tools_condition

#LLM 정의와 도구 결합
llm = ChatOpenAI(model="gpt-4.1-nano",temperature=0)
llm_with_tools = llm.bind_tools(tools)

#state
class State(TypedDict):
  messages: Annotated[list, add_messages]

# chatbot 노드에서 처리할 함수 정의
def chatbot(state:State):
  # Message 반환
  return {"messages":[llm.invoke(state["messages"])]}

#graph 생성
graph_builder = StateGraph(State)
# node 추가
graph_builder.add_node("chatbot",chatbot)
# ADD Tool Node
graph_builder.add_node("tools",ToolNode)
# 시작 노드에서 챗봇 노드로 엣지 추가
graph_builder.add_edge(START, "chatbot")
# 조건부 연걸 엣지 추가
graph_builder.add_conditional_edges(
    "chatbot", # 출발노드
    tools_condition, # 조건함수
    # 조건에 따른 도착 노드도 필요
    )
graph_builder.add_edge("tools","chatbot")
# 챗봇에서 END로 연결
graph_builder.add_edge("chatbot",END)

# 그래프 컴파일
graph= graph_builder.compile()

show_graph(graph)

question = "앤트로픽에 투자한 기업과 투자금액은?"
for event in graph.stream({"messages":[("user",question)]}):
  for value in event.values():
    value["messages"][-1].pretty_print()