# -*- coding: utf-8 -*-
"""RAG_reinforcemance_yjyoo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PwuOqR-0R8NWp2BI8oAnpievDVWGrg0K

# 성능 올리기
1. 마크다운 추출 청크

2. 리트리버(앙상블, parent-chidren)

3. 리랭커
"""

!pip install numpy==1.26.4
!pip install langchain==0.3.21
!pip install langchain-core==0.3.46
!pip install langchain-experimental==0.3.4
!pip install langchain-community==0.3.20
!pip install langchain-openai==0.3.9
!pip install langchain-chroma==0.2.2
!pip install langchain-cohere==0.4.3
!pip install langchain-milvus==0.1.8
#!pip install langgraph==0.3.18
#!pip install langsmith==0.3.18
!pip install pymupdf==1.25.4
!pip install pypdf==4.3.1
!pip install pdfplumber==0.11.5
!pip install faiss-cpu==1.10.0
#!pip install langchain-teddynote==0.3.44

import os
my_key=
os.environ["OPENAI_API_KEY"]=my_key

import numpy as np
np.__version__

"""# 출처 표기까지 체인 구성"""

from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import  RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain_openai import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import FAISS
# 문서로드
file_name="/content/booksv_02.csv"
from langchain.document_loaders.csv_loader import CSVLoader
loader = CSVLoader(file_name)
docs=loader.load()
print(len(docs))
# 문서 청크분할

# 임베딩 모델 설정
embedding = OpenAIEmbeddings()
# db 설정
db = FAISS.from_documents(docs, embedding)

# 리트리버 설정
retriever = db.as_retriever(search_type="mmr",search_kwargs={"k":5,"fetch_k":7})

# 프롬프트를 생성합니다.
from langchain.prompts import PromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough
prompt = PromptTemplate.from_template(
    """You are an assistant for question-answering tasks.
Use the following pieces of retrieved context to answer the question.
If you don't know the answer, just say that you don't know.

#Context:
{context}

#Question:
{question}

#Answer:"""
)
llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)
# 체인(Chain) 생성
chain = {"context":retriever, "question":RunnablePassthrough()} | prompt | llm | StrOutputParser()

chain.invoke("Gilead의 작가는?")

from langchain.chains import RetrievalQAWithSourcesChain

# stuff로
chain = RetrievalQAWithSourcesChain.from_chain_type(llm = llm, retriever = retriever, chain_type = "stuff")
chain.invoke("Gilead의 작가는?")

# 대화를 기억하는 기능
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)
chain = ConversationalRetrievalChain.from_llm(llm, retriever=retriever)
chain.invoke({"question": "Gilead의 작가는?", "chat_history": []})

"""# Retriever

## 앙상블 = Hybrid
"""

# Lexical Search 를 위한 install
!pip install rank_bm25

from langchain.retrievers import BM25Retriever, EnsembleRetriever
from langchain.vectorstores import FAISS
from langchain_chroma import Chroma
from langchain_openai import OpenAIEmbeddings

# 샘플 문서 리스트
doc_list = [
    "I like apples",
    "I like apple company",
    "I like apple's iphone",
    "Apple is my favorite company",
    "I like apple's ipad",
    "I like apple's macbook",
]

# BM25Retriever 초기화
lexical_retriever = BM25Retriever.from_texts(doc_list)
lexical_retriever.k=1

# Vector 기반 retriever 생성
cdb1 = Chroma.from_texts(doc_list, OpenAIEmbeddings())
cdb_retriever = cdb1.as_retriever(search_kwargs={"k":1})

# Hybrid 설정 가중치 설정 가능
enc_retriever = EnsembleRetriever(retrievers=[lexical_retriever, cdb_retriever], weights=[0.5,0.5])

q="My favorite fruit is apple"
docs = enc_retriever.get_relevant_documents(q)
print(docs)

"""## Parent Document Retriever

**문서 검색과 문서 분할의 균형 잡기**

문서 검색 과정에서 문서를 적절한 크기의 조각(청크)으로 나누는 것은 다음의 **상충되는 두 가지 중요한 요소를 고려**해야 합니다.

1. 작은 문서를 원하는 경우: 이렇게 하면 문서의 임베딩이 그 의미를 가장 정확하게 반영할 수 있습니다. 문서가 너무 길면 임베딩이 의미를 잃어버릴 수 있습니다.
2. 각 청크의 맥락이 유지되도록 충분히 긴 문서를 원하는 경우입니다.

**`ParentDocumentRetriever`의 역할**

이 두 요구 사항 사이의 균형을 맞추기 위해 `ParentDocumentRetriever`라는 도구가 사용됩니다. 이 도구는 문서를 작은 조각으로 나누고, 이 조각들을 관리합니다. 검색을 진행할 때는, 먼저 이 작은 조각들을 찾아낸 다음, 이 조각들이 속한 원본 문서(또는 더 큰 조각)의 식별자(ID)를 통해 전체적인 맥락을 파악할 수 있습니다.

여기서 '부모 문서'란, 작은 조각이 나누어진 원본 문서를 말합니다. 이는 전체 문서일 수도 있고, 비교적 큰 다른 조각일 수도 있습니다. 이 방식을 통해 문서의 의미를 정확하게 파악하면서도, 전체적인 맥락을 유지할 수 있게 됩니다.

**정리**

- **문서 간의 계층 구조 활용**: `ParentDocumentRetriever`는 문서 검색의 효율성을 높이기 위해 문서 간의 계층 구조를 활용합니다.
- **검색 성능 향상**: 관련성 높은 문서를 빠르게 찾아내며, 주어진 질문에 대한 가장 적합한 답변을 제공하는 문서를 효과적으로 찾아낼 수 있습니다.
  문서를 검색할 때 자주 발생하는 두 가지 상충되는 요구 사항이 있습니다:

"""

from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.document_loaders import TextLoader

loaders = [TextLoader("/content/appendix-keywords.txt")]
docs =[]
for loader in loaders:
  docs.extend(loader.load())

parent_splitter = RecursiveCharacterTextSplitter(chunk_size=1000)
child_splitter = RecursiveCharacterTextSplitter(chunk_size=200)

# 자식 청크를 인덱싱 하는데 사용할 벡터 저장소
vectorstore= Chroma.from_documents(docs,OpenAIEmbeddings(),collection_name="split_parent")

# 부모 문서 저장 계층
from langchain.storage import InMemoryStore
store= InMemoryStore()

from langchain.retrievers import ParentDocumentRetriever
# 저장은 Vector 자식청크, 꺼내오는 곳은 부모 Store
p_retriever = ParentDocumentRetriever(vectorstore=vectorstore, docstore = store, child_splitter=child_splitter, parent_splitter=parent_splitter)

p_retriever.add_documents(docs)

#유사도 검사
sub_text = vectorstore.similarity_search_with_score("word2vec에 대해서 알려줄래?")
print(sub_text[0][0].page_content)
print("*"*50)
retriever_text =  p_retriever.invoke("word2vec에 대해서 알려줄래?")
print(retriever_text[0].page_content)

"""- `RecursiveCharacterTextSplitter`를 사용하여 부모 문서와 자식 문서를 생성합니다.
  - 부모 문서는 `chunk_size`가 1000으로 설정되어 있습니다.
  - 자식 문서는 `chunk_size`가 200으로 설정되어 있으며, 부모 문서보다 작은 크기로 생성됩니다.

# 문맥 압축 검색기(ContextualCompressionRetriever)

검색 시스템에서 직면하는 어려움 중 하나는 데이터를 시스템에 수집할 때 어떤 특정 질의를 처리해야 할지 미리 알 수 없다는 점입니다.

이는 질의와 가장 관련성이 높은 정보가 많은 양의 무관한 텍스트를 포함한 문서에 묻혀 있을 수 있음을 의미합니다.

이러한 전체 문서를 애플리케이션에 전달하면 더 비용이 많이 드는 LLM 호출과 품질이 낮은 응답으로 이어질 수 있습니다.

`ContextualCompressionRetriever` 은 이 문제를 해결하기 위해 고안되었습니다.

아이디어는 간단합니다. 검색된 문서를 그대로 즉시 반환하는 대신, 주어진 질의의 맥락을 사용하여 문서를 압축함으로써 관련 정보만 반환되도록 할 수 있습니다.

여기서 "압축"은 개별 문서의 내용을 압축하는 것과 문서를 전체적으로 필터링하는 것 모두를 의미합니다.

`ContextualCompressionRetriever` 는 질의를 base retriever에 전달하고, 초기 문서를 가져와 Document Compressor를 통과시킵니다.

Document Compressor는 문서 목록을 가져와 문서의 내용을 줄이거나 문서를 완전히 삭제하여 목록을 축소합니다.

> 출처: https://drive.google.com/uc?id=1CtNgWODXZudxAWSRiWgSGEoTNrUFT98v

![](./images/01-Contextual-Compression.jpeg)
"""

# 문서를 예쁘게 출력하기 위한 도우미 함수
def pretty_print_docs(docs):
    print(
        f"\n{'-' * 100}\n".join(
            [f"문서 {i+1}:\n\n" + d.page_content for i, d in enumerate(docs)]
        )
    )

from langchain_community.vectorstores import FAISS
loaders = [TextLoader("/content/appendix-keywords.txt")]
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300)
docs = loader.load_and_split(text_splitter)
retriever = FAISS.from_documents(docs, OpenAIEmbeddings()).as_retriever()

context = retriever.invoke("Sementic search에 대해 알려줘")

pretty_print_docs(context)

!pip install langchain-teddynote==0.3.44

from langchain_teddynote.document_compressors import LLMChainExtractor
from langchain.retrievers import ContextualCompressionRetriever

# LLM을 사용하여 문서 압축기 생성
compressor = LLMChainExtractor.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)

compression_retriever.invoke("Sementic search에 대해 알려줘")



"""# Cross Encoder Reranker

## 개요

Cross encoder reranker는 검색 증강 생성(RAG) 시스템의 성능을 향상시키기 위해 사용되는 기술입니다. 이 문서는 Hugging Face의 cross encoder 모델을 사용하여 retriever에서 reranker를 구현하는 방법을 설명합니다.

## 주요 특징 및 작동 방식

1. **목적**: 검색된 문서들의 순위를 재조정하여 질문에 가장 관련성 높은 문서를 상위로 올림
2. **구조**: 질문과 문서를 동시에 입력으로 받아 처리
3. **작동 방식**:
  - 질문과 문서를 하나의 입력으로 사용하여 유사도를 직접 출력
  - Self-attention 메커니즘을 통해 질문과 문서를 동시에 분석
4. **장점**:
  - 더 정확한 유사도 측정 가능
  - 질문과 문서 사이의 의미론적 유사성을 깊이 탐색
5. **한계점**:
  - 연산 비용이 높고 시간이 오래 걸림
  - 대규모 문서 집합에 직접 적용하기 어려움

## 실제 사용

- 일반적으로 초기 검색에서 상위 k개의 문서에 대해서만 reranking 수행
- Bi-encoder로 빠르게 후보를 추출한 후, Cross encoder로 정확도를 높이는 방식으로 활용

## 구현

- Hugging Face의 cross encoder 모델 또는 BAAI/bge-reranker와 같은 모델 사용
- LangChain 등의 프레임워크에서 CrossEncoderReranker 컴포넌트를 통해 쉽게 통합 가능

## Reranker의 주요 장점

1. 더 정확한 유사도 측정
2. 심층적인 의미론적 유사성 탐색
3. 검색 결과 개선
4. RAG 시스템 성능 향상
5. 유연한 통합
6. 다양한 사전 학습 모델 선택 가능

## Reranker 사용 시 문서 수 설정

- 일반적으로 상위 5~10개 문서에 대해 reranking 수행
- 최적의 문서 수는 실험과 평가를 통해 결정 필요

## Reranker 사용시 Trade-offs

1. 정확도 vs 처리 시간
2. 성능 향상 vs 계산 비용
3. 검색 속도 vs 관련성 정확도
4. 시스템 요구사항 충족
5. 데이터셋 특성 고려

이제 기본 `retriever`를 `ContextualCompressionRetriever`로 감싸보겠습니다. `CrossEncoderReranker`는 `HuggingFaceCrossEncoder`를 사용하여 반환된 결과를 재정렬합니다.
"""

!pip install sentence-transformers

from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder

# 벡터 검색 결과
context = retriever.invoke("word2vec에 대해 알려줘")
pretty_print_docs(context)
print("*"*50)
# Re ordering
reranker_model = HuggingFaceCrossEncoder(model_name="BAAI/bge-reranker-v2-m3")

# 리 랭킹을 컴프레서로 하는 것 -> HuggingFace 모델에 넣었다 빼서 시간이 걸림
compressor = CrossEncoderReranker(model=reranker_model)
reranker_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=retriever)

#결과
pretty_print_docs(reranker_retriever.invoke("word2vec에 대해 알려줘"))

"""## AGENT

## 기본적인 Tool 사용 Agent
"""

# 에이전트와 도구 준비
from langchain.agents import load_tools
from langchain.agents import initialize_agent
from langchain.agents import AgentType
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini",temperature=0)
tools=load_tools(["llm-math"],llm=llm)

agent = initialize_agent(tools, llm, agent="zero-shot-react-description")

print(agent.run("대한민국의 수도는?"))

agent = initialize_agent(tools, llm, agent="zero-shot-react-description",verbose=True)

# print(agent.run("대한민국의 수도는?"))
print(agent.run("3x+4=5 의 해는?"))

# Code Interpreter
from langchain_experimental.tools import PythonAstREPLTool
python_repl_tool = PythonAstREPLTool()
math_tool = load_tools(["llm-math"],llm=llm)[0]
agent= initialize_agent([python_repl_tool,math_tool],llm,agent="zero-shot-react-description",verbose=True)

print(agent.run("피보나치 수열을 계산하는 파이썬 코드 작성"))

# google search 도구를 위한 모듈 설치
!pip install google-search-results

serp_key =
os.environ["SERPAPI_API_KEY"] =serp_key

# 웹 검색 도구
# 과거 검색 내역 메모리에 저장
from langchain.memory import ConversationBufferMemory

llm = ChatOpenAI(model="gpt-4o-mini",temperature=0)
python_repl_tool = PythonAstREPLTool()

tools = load_tools(["serpapi","llm-math"],llm=llm)
tools.append(python_repl_tool)
agent= initialize_agent(tools,llm,agent="conversational-react-description",memory=ConversationBufferMemory(memory_key="chat_history"),verbose=True)

#print(agent.run("2025년 교황의 이름은?"))

agent.run('''나는 30개의 초콜렛을 사려해. 100원 이었던 가격이 15% 인상했어. 얼마가 필요해?''')

agent.run('''근데 내가 5000원을 가지고 있다면 얼마를 거슬러받아?''')

"""# RAG를 수행하는 AGENT"""

# Retriever 구성
from langchain.schema import HumanMessage
from langchain_community.document_loaders import PyMuPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS

loader = PyMuPDFLoader("/content/국회회의록_22대_422회_1차_문화체육관광위원회.PDF")
document = loader.load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=20)
docs = text_splitter.split_documents(document)
db = FAISS.from_documents(docs, OpenAIEmbeddings())

my_retriever= db.as_retriever()

#도구(Tool) 정의
from langchain.tools.retriever import create_retriever_tool
retriever_tool = create_retriever_tool(retriever=my_retriever, name="pdf_search", description="use this tool to search information from pdf document")
tools = load_tools(["llm-math","serpapi"],llm)
tools.append(retriever_tool)

from langchain_core.prompts import ChatPromptTemplate
prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a helpful assistant."
            "Make sure to use the `pdf_search` tool for searching information from the PDF document. You have to tell where(which tool) you get the information when you invoke"
            "If you can't find the information from the PDF document, use the `search` tool or 'calculator' for searching information from the web."


),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
        ("placeholder", "{agent_scratchpad}"),
    ]
)

from langchain.agents import AgentExecutor
from langchain.agents import create_tool_calling_agent
agent = create_tool_calling_agent(llm=llm, tools=tools, prompt=prompt)
agent_excutor= AgentExecutor(agent=agent,tools=tools, verbose=True)

agent_excutor.invoke({"input":"회의 참석자의 이름은?"})

"""#그라디오 GRADIO"""

!pip install gradio

# Gradio
import gradio as gr
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough

llm = ChatOpenAI(model="gpt-4o-mini",temperature=0)
prompt = PromptTemplate.from_template("{question}")
chain = prompt | llm | StrOutputParser()

def greet(question):
    result = chain.invoke(question)
    return result

demo = gr.Interface(
    fn=greet,
    inputs=["text"],
    outputs=["text"],
)

demo.launch()

