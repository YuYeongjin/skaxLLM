# -*- coding: utf-8 -*-
"""llm_전처리_yjyoo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jZB6Rv7q9rEvZo5Mxr2ACOVx8nP2IKG8

# 토큰화
"""

!pip install konlpy

from konlpy.tag import Okt
okt = Okt()

text="이것은 한국어를 토큰화하는 예제입니다."
print(okt.morphs(text))
len(okt.morphs(text))
# 자연어 토큰화

import tiktoken
enc = tiktoken.encoding_for_model("gpt-3.5-turbo")
tokens = enc.encode(text)
print(tokens)
print(len(tokens))
# 토큰화 된 것을 숫자형으로 인코딩

# 정수를 자연어로 역 추적 (Decode)
b_tokens = enc.decode_tokens_bytes(tokens)
print(b_tokens)

# 디코드 시 에러 발생하면 그대로 출력하기
def data2str(data) :
  try:
    return data.decode("utf-8")
  except:
    return data

b'\xec\x9d\xb4'.decode("utf-8")
print([data2str(data) for data in b_tokens])

# gpt-4o 버전 토큰화툴 호출
enc2 = tiktoken.encoding_for_model("gpt-4o")
tokens2 = enc2.encode(text)
print(tokens2)
print(len(tokens2))

b_tokens2 = enc2.decode_tokens_bytes(tokens2)
print([data2str(data) for data in b_tokens2])

"""# 자연어 전처리 과정

1. 토큰화
2. 정수 인코딩 ( 단어 인덱스 사전 => 토큰에 인덱스 할당 )
3. 문장의 길이 맞추기 = 패딩 ( 빈자리 0 으로 채우기 )
4. one hot encodeing ( 희소벡터 ) => 임베딩 ( 유사성을 반영한 실수 형태의 Dense Vector ) => 모델에 내장되어 있음
"""

texts=["좋아하는 음식은 무엇인가요?",
              "어디에 살고 계세요?",
              "오늘은 기분이 어떻세요?",
              "길이 많이 막히네요",
              "날씨가 덥습니다"]

from konlpy.tag import Okt
okt = Okt()
# text list 토큰화

tokens = [ okt.morphs(text) for text in texts]
print(tokens)

# 정수 인코딩 ( 단어 인덱스 사전 => 토큰에 인덱스 할당)
# 텐서플로
from tensorflow.keras.preprocessing.text import Tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(tokens)
print(tokenizer.word_index)
x= tokenizer.texts_to_sequences(tokens)
print(x)

# 길이를 통일 시키는 Padding
from tensorflow.keras.preprocessing.sequence import pad_sequences
x= pad_sequences(x,padding="post")
print(x)